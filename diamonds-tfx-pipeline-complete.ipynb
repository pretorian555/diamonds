{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8-FJCsrO4v0"
   },
   "source": [
    "## Local runner pipeline diamonds model notebook\n",
    "\n",
    "the goal of this notebook is to develop a code for a local pipeline run. \n",
    "\n",
    "the structure:\n",
    "\n",
    "    -diamonds directory - holds all the code, including the notebook code\n",
    "    -pipeline directory - holds the python code for running the pipeline\n",
    "    -model directory - holds the code for running the pipeline components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXkThMFVFRXJ"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ACTfjt_PPndL"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it exists\n"
     ]
    }
   ],
   "source": [
    "data = 'gs://diamonds_data'\n",
    "\n",
    "if data:\n",
    "    print(\"it exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\n",
      "drwxr-xr-x 4 jupyter jupyter 4096 Feb 25 00:54 \u001b[0m\u001b[01;34mstored_model\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls -l ~/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "thgFqLvvOcZt"
   },
   "outputs": [],
   "source": [
    "##create a directory for all the pipeline code\n",
    "\n",
    "##pipeline_home_directory = '~/diamonds'\n",
    "\n",
    "##os.makedirs(pipeline_home_directory, exist_ok=True)\n",
    "\n",
    "components_directory = 'model'\n",
    "os.makedirs(components_directory, exist_ok=True)\n",
    "\n",
    "pipeline_directory = 'pipeline'\n",
    "os.makedirs(pipeline_directory, exist_ok=True)\n",
    "\n",
    "data_directory = 'gs://diamonds_data/'\n",
    "\n",
    "if not data_directory:\n",
    "    data_directory = os.path.join('~', 'data')\n",
    "    os.makedirs(data_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "154hyjTvS82G"
   },
   "source": [
    "## write all the pipeline components code to python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1647273958304,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "8rKvXAWchUQx",
    "outputId": "ddb37ff2-cd2b-4c95-ac4c-5cb2e6fb8b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/__init__.py\n",
    "\n",
    "##module init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1647273958305,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "mCQjT3RsdPdS",
    "outputId": "a13a8af8-561d-4072-cc29-88244d8eea3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/features.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile model/features.py\n",
    "\n",
    "NUMERICAL_FEATURES = ['carat','depth','table','x','y','z']\n",
    "\n",
    "CATEGORICAL_FEATURES = ['clarity','color','cut']\n",
    "\n",
    "LABEL_KEY = 'price'\n",
    "\n",
    "\n",
    "def transformed_name(key):\n",
    "  return key + '_xf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1647273958305,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "hzXX0tD4Snu9",
    "outputId": "aad665e0-d257-40b9-b10b-ca96e8ab4243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/preprocessing.py\n",
    "## we are going to preprocess our data through Transform library\n",
    "\n",
    "## we will need to define a preprocessing module (a python function that will preprocess the data)\n",
    "\n",
    "##the Transform component is looking for preprocessing_fn  in the module \n",
    "\n",
    "## steps to define preprocessing function\n",
    "\n",
    "## 1. define list of columns based on the data they contain (numerical, cathegorical, etc )\n",
    "\n",
    "## 2. define a preprocessing_fn\n",
    "\n",
    "##this is a preprocessing function based on Transform tfx module\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import importlib\n",
    "\n",
    "from model import features\n",
    "importlib.reload(features)\n",
    "\n",
    "##import pipeline.features as features\n",
    "\n",
    "\n",
    "##create a list of feature names here\n",
    "\n",
    "def transformed_name(key):\n",
    "  return key + '_xf'\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "  \"\"\"Replace missing values in a SparseTensor.\n",
    "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
    "  Args:\n",
    "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
    "      in the second dimension.\n",
    "  Returns:\n",
    "    A rank 1 tensor where missing values of `x` have been filled in.\n",
    "  \"\"\"\n",
    "  default_value = '' if x.dtype == tf.string else 0\n",
    "  return tf.squeeze(\n",
    "      tf.sparse.to_dense(\n",
    "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "          default_value),\n",
    "      axis=1)\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "\n",
    "  ##create outputs out of inputs \n",
    "  outputs = {}\n",
    "\n",
    "  ##preprocess numerical \n",
    "  for feature in features.NUMERICAL_FEATURES:\n",
    "    ##we have to use dense tensors in our keras layers \n",
    "    outputs[transformed_name(feature)] = tft.scale_to_z_score(_fill_in_missing(inputs[feature]))\n",
    "\n",
    "  for feature in features.CATEGORICAL_FEATURES:\n",
    "    outputs[transformed_name(feature)] = tft.compute_and_apply_vocabulary(\n",
    "        x = _fill_in_missing(inputs[feature]),\n",
    "        num_oov_buckets=1,\n",
    "        vocab_filename=feature\n",
    "    )  \n",
    "\n",
    "  ##for feature in features.LABEL_KEY:\n",
    "    feature = features.LABEL_KEY\n",
    "    outputs[transformed_name(feature)] = _fill_in_missing(inputs[feature])\n",
    "\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1647273958305,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "H81mnYMiTaSM",
    "outputId": "eff07f9b-ae3b-438d-94b3-f35c6655b661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/model.py\n",
    "\n",
    "##trainer and tuner module\n",
    "\n",
    "import functools\n",
    "import absl\n",
    "import os\n",
    "from typing import List, Text\n",
    "\n",
    "\n",
    "import kerastuner\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx.components.trainer.executor import TrainerFnArgs\n",
    "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
    "from tfx.components.tuner.component import TunerFnResult\n",
    "from tfx_bsl.tfxio import dataset_options\n",
    "\n",
    "import importlib\n",
    "\n",
    "##import pipeline.features as features\n",
    "from model import features\n",
    "importlib.reload(features)\n",
    "\n",
    "EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "  \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "  return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "  \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
    "\n",
    "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "  @tf.function\n",
    "  def serve_tf_examples_fn(serialized_tf_examples):\n",
    "    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "    feature_spec = tf_transform_output.raw_feature_spec()\n",
    "    feature_spec.pop(features.LABEL_KEY)\n",
    "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "\n",
    "    transformed_features = model.tft_layer(parsed_features)\n",
    "\n",
    "    return model(transformed_features)\n",
    "\n",
    "  return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              data_accessor: DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  dataset = data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      dataset_options.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=features.transformed_name(features.LABEL_KEY)),\n",
    "      tf_transform_output.transformed_metadata.schema)\n",
    "    \n",
    "  return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _get_hyperparameters() -> kerastuner.HyperParameters:\n",
    "  \"\"\"Returns hyperparameters for building Keras model.\"\"\"\n",
    "  hp = kerastuner.HyperParameters()\n",
    "  # Defines search space.\n",
    "  hp.Choice('learning_rate', [1e-1,1e-2, 1e-3], default=1e-3)\n",
    "  hp.Int('n_layers', 1, 2, default=1)\n",
    "  with hp.conditional_scope('n_layers', 1):\n",
    "        hp.Int('n_units_1', min_value=8, max_value=128, step=8, default=8)\n",
    "  with hp.conditional_scope('n_layers', 2):\n",
    "        hp.Int('n_units_1', min_value=8, max_value=128, step=8, default=8)\n",
    "        hp.Int('n_units_2', min_value=8, max_value=128, step=8, default=8)        \n",
    "\n",
    "  return hp\n",
    "\n",
    "\n",
    "def _build_keras_model(hparams: kerastuner.HyperParameters, \n",
    "  tf_transform_output: tft.TFTransformOutput) -> tf.keras.Model:\n",
    "  \"\"\"Creates a Keras WideDeep Classifier model.\n",
    "  Args:\n",
    "    hparams: Holds HyperParameters for tuning.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "  Returns:\n",
    "    A keras Model.\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  we are going to skip building feature columns for now\n",
    "  deep_columns = [\n",
    "      tf.feature_column.numeric_column(\n",
    "          key=features.transformed_name(key), \n",
    "          shape=())\n",
    "      for key in features.NUMERIC_FEATURE_KEYS\n",
    "  ]\n",
    "  \"\"\"\n",
    "\n",
    "  ##creating a list of sparse inputs, specifically indicating it during the input creation  \n",
    "  input_layers = [\n",
    "      tf.keras.layers.Input(name=features.transformed_name(column), shape=(1,))\n",
    "      for column in features.NUMERICAL_FEATURES + features.CATEGORICAL_FEATURES\n",
    "  ]    \n",
    "  print(\"is this ident error too?\")\n",
    "  \"\"\"\n",
    "  also skipping these feature columns\n",
    "  categorical_columns = [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key=features.transformed_name(key), \n",
    "          num_buckets=tf_transform_output.num_buckets_for_transformed_feature(features.transformed_name(key)), \n",
    "          default_value=0)\n",
    "      for key in features.CATEGORICAL_FEATURE_KEYS\n",
    "  ]\n",
    "\n",
    "  wide_columns = [\n",
    "      tf.feature_column.indicator_column(categorical_column)\n",
    "      for categorical_column in categorical_columns\n",
    "  ]\n",
    "    \n",
    "  input_layers.update({\n",
    "      column.categorical_column.key: tf.keras.layers.Input(name=column.categorical_column.key, shape=(), dtype=tf.int32)\n",
    "      for column in wide_columns\n",
    "  })\n",
    "  \"\"\"\n",
    "\n",
    "  ##  deep = tf.keras.layers.DenseFeatures(deep_columns)(input_layers)\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  for n in range(int(hparams.get('n_layers'))):\n",
    "    deep = tf.keras.layers.Dense(units=hparams.get('n_units_' + str(n + 1)))(deep)\n",
    "\n",
    "  wide = tf.keras.layers.DenseFeatures(wide_columns)(input_layers)\n",
    "\n",
    "  output = tf.keras.layers.Dense(features.NUM_CLASSES, activation='softmax')(\n",
    "               tf.keras.layers.concatenate([deep, wide]))\n",
    "\n",
    "  \"\"\"\n",
    "  concat_inputs = tf.keras.layers.concatenate(input_layers)\n",
    "\n",
    "  for n in range(int(hparams.get('n_layers'))):\n",
    "    x = tf.keras.layers.Dense(units=hparams.get('n_units_' + str(n +1)), activation='relu')(concat_inputs)\n",
    " \n",
    "  output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "  model = tf.keras.Model(input_layers, output)\n",
    "\n",
    "  ##since this is a regression model, we are using\n",
    "  ## loss = \n",
    "  model.compile(\n",
    "      ##commented out stuff is from a template for classification\n",
    "      ##loss='sparse_categorical_crossentropy',\n",
    "      loss='mean_squared_error',\n",
    "      optimizer=tf.keras.optimizers.Adam(lr=hparams.get('learning_rate')),\n",
    "      ##metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "      metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "  model.summary(print_fn=absl.logging.info)\n",
    "\n",
    "  return model    \n",
    "\n",
    "\n",
    "# TFX Tuner will call this function.\n",
    "def tuner_fn(fn_args: TrainerFnArgs) -> TunerFnResult:\n",
    "  \"\"\"Build the tuner using the KerasTuner API.\n",
    "  Args:\n",
    "    fn_args: Holds args as name/value pairs.\n",
    "      - working_dir: working dir for tuning.\n",
    "      - train_files: List of file paths containing training tf.Example data.\n",
    "      - eval_files: List of file paths containing eval tf.Example data.\n",
    "      - train_steps: number of train steps.\n",
    "      - eval_steps: number of eval steps.\n",
    "      - schema_path: optional schema of the input data.\n",
    "      - transform_graph_path: optional transform graph produced by TFT.\n",
    "  Returns:\n",
    "    A namedtuple contains the following:\n",
    "      - tuner: A BaseTuner that will be used for tuning.\n",
    "      - fit_kwargs: Args to pass to tuner's run_trial function for fitting the\n",
    "                    model , e.g., the training and validation dataset. Required\n",
    "                    args depend on the above tuner's implementation.\n",
    "  \"\"\"\n",
    "  transform_graph = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "  \n",
    "  # Construct a build_keras_model_fn that just takes hyperparams from get_hyperparameters as input.\n",
    "  build_keras_model_fn = functools.partial(\n",
    "      _build_keras_model, tf_transform_output=transform_graph)  \n",
    "\n",
    "  # BayesianOptimization is a subclass of kerastuner.Tuner which inherits from BaseTuner.   \n",
    "  tuner = kerastuner.BayesianOptimization(\n",
    "      build_keras_model_fn,\n",
    "      max_trials=20,\n",
    "      overwrite=True,\n",
    "      hyperparameters=_get_hyperparameters(),\n",
    "      # New entries allowed for n_units hyperparameter construction conditional on n_layers selected.\n",
    "#       allow_new_entries=True,\n",
    "#       tune_new_entries=True,\n",
    "      ##objective=kerastuner.Objective('val_sparse_categorical_accuracy', 'max'),\n",
    "      objective=kerastuner.Objective('val_loss', 'min'),\n",
    "      distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "      directory=fn_args.working_dir,\n",
    "      project_name='diamond_price_tuning')\n",
    "  \n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      transform_graph,\n",
    "      batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      transform_graph,\n",
    "      batch_size=EVAL_BATCH_SIZE)\n",
    "\n",
    "  return TunerFnResult(\n",
    "      tuner=tuner,\n",
    "      fit_kwargs={\n",
    "          'epochs':EPOCHS,\n",
    "          'x': train_dataset,\n",
    "          'validation_data': eval_dataset,\n",
    "          'steps_per_epoch': fn_args.train_steps,\n",
    "          'validation_steps': fn_args.eval_steps,\n",
    "          'callbacks':[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "      })\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: TrainerFnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files, \n",
    "      fn_args.data_accessor, \n",
    "      tf_transform_output, \n",
    "      TRAIN_BATCH_SIZE)\n",
    "\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files, \n",
    "      fn_args.data_accessor,\n",
    "      tf_transform_output, \n",
    "      EVAL_BATCH_SIZE)\n",
    "\n",
    "  if fn_args.hyperparameters:\n",
    "    hparams = kerastuner.HyperParameters.from_config(fn_args.hyperparameters)\n",
    "  else:\n",
    "    # This is a shown case when hyperparameters is decided and Tuner is removed\n",
    "    # from the pipeline. User can also inline the hyperparameters directly in\n",
    "    # _build_keras_model.\n",
    "    hparams = _get_hyperparameters()\n",
    "  absl.logging.info('HyperParameters for training: %s' % hparams.get_config())\n",
    "  \n",
    "  # Distribute training over multiple replicas on the same machine.\n",
    "  mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "  with mirrored_strategy.scope():\n",
    "      model = _build_keras_model(\n",
    "            hparams=hparams,\n",
    "            tf_transform_output=tf_transform_output)\n",
    "\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=fn_args.model_run_dir, update_freq='batch')\n",
    "  \n",
    "  earlystopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "      monitor='val_loss', patience=3, mode='min'\n",
    "  )\n",
    "\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      epochs=EPOCHS,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps,\n",
    "      callbacks=[tensorboard_callback,earlystopping_callback])\n",
    "    \n",
    "  signatures = {\n",
    "      'serving_default':\n",
    "          _get_serve_tf_examples_fn(model,\n",
    "                                    tf_transform_output).get_concrete_function(\n",
    "                                        tf.TensorSpec(\n",
    "                                            shape=[None],\n",
    "                                            dtype=tf.string,\n",
    "                                            name='examples')),\n",
    "  }\n",
    "  \n",
    "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1647273958306,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "9dN474QZhqR7",
    "outputId": "7cda0143-0262-41bf-aea0-30995d255f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline/__init__.py\n",
    "\n",
    "##module init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1647273958306,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "nV4Wyor9U46k",
    "outputId": "c3ce48d9-4621-4f7f-f4a1-c1e74e5437e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline/configs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline/configs.py\n",
    "\n",
    "import os  # pylint: disable=unused-import\n",
    "\n",
    "# TODO(b/149347293): Move more TFX CLI flags into python configuration.\n",
    "\n",
    "# Pipeline name will be used to identify this pipeline.\n",
    "PIPELINE_NAME = 'diamonds-pipeline'\n",
    "\n",
    "# GCP related configs.\n",
    "\n",
    "# Following code will retrieve your GCP project. You can choose which project\n",
    "# to use by setting GOOGLE_CLOUD_PROJECT environment variable.\n",
    "\n",
    "\"\"\"\n",
    "try:\n",
    "  import google.auth  # pylint: disable=g-import-not-at-top  # pytype: disable=import-error\n",
    "  try:\n",
    "    _, GOOGLE_CLOUD_PROJECT = google.auth.default()\n",
    "  except google.auth.exceptions.DefaultCredentialsError:\n",
    "    GOOGLE_CLOUD_PROJECT = ''\n",
    "except ImportError:\n",
    "  GOOGLE_CLOUD_PROJECT = ''\n",
    "\"\"\"\n",
    "# Specify your GCS bucket name here. You have to use GCS to store output files\n",
    "# when running a pipeline with Kubeflow Pipeline on GCP or when running a job\n",
    "# using Dataflow. Default is '<gcp_project_name>-kubeflowpipelines-default'.\n",
    "# This bucket is created automatically when you deploy KFP from marketplace.\n",
    "#GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + '-kubeflowpipelines-default'\n",
    "\n",
    "# Following image will be used to run pipeline components run if Kubeflow\n",
    "# Pipelines used.\n",
    "# This image will be automatically built by CLI if we use --build-image flag.\n",
    "#PIPELINE_IMAGE = f'gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}'\n",
    "\n",
    "PREPROCESSING_FN = 'model.preprocessing.preprocessing_fn'\n",
    "RUN_FN = 'model.model.run_fn'\n",
    "TUNER_FN = 'model.model.tuner_fn'\n",
    "\n",
    "TRAIN_NUM_STEPS = 5000\n",
    "EVAL_NUM_STEPS = 1000\n",
    "\n",
    "# Change this value according to your use cases.\n",
    "EVAL_ACCURACY_THRESHOLD = 0.6\n",
    "\n",
    "# Google Cloud BigQuery related configs.\n",
    "# Use following configs to use BigQueryExampleGen as a data source.\n",
    "#\n",
    "# Beam args to use BigQueryExampleGen with Beam DirectRunner.\n",
    "#\n",
    "# BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS = [\n",
    "#    '--project=' + GOOGLE_CLOUD_PROJECT,\n",
    "#    '--temp_location=' + os.path.join('gs://', GCS_BUCKET_NAME, 'tmp'),\n",
    "#    ]\n",
    "\n",
    "# The query that extracts the examples from BigQuery.\n",
    "#\n",
    "# BIG_QUERY_QUERY = \"\"\"\n",
    "#         SELECT ...\n",
    "#         FROM\n",
    "#         WHERE\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1647282289114,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "W7UHgH5ZVh2-",
    "outputId": "705e8fd4-a55d-4d2b-ebdb-599d69e37872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline/pipeline.py\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tfx\n",
    "from model import features\n",
    "\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "\n",
    "##local variables for modules\n",
    "\n",
    "def create_pipeline(\n",
    "    pipeline_name: str,\n",
    "    pipeline_root: str,\n",
    "    data_path: str,\n",
    "    preprocessing_fn: str,\n",
    "    run_fn: str,\n",
    "    tuner_fn: str,\n",
    "    enable_tuning: bool,\n",
    "    train_args: tfx.v1.proto.TrainArgs,\n",
    "    eval_args: tfx.v1.proto.EvalArgs,\n",
    "    eval_accuracy_threshold: float,\n",
    "    serving_model_dir: str,\n",
    "    schema_path: Optional[str] = None,\n",
    "    metadata_connection_config: Optional[\n",
    "        metadata_store_pb2.ConnectionConfig] = None,\n",
    "    beam_pipeline_args: Optional[List[str]] = None,\n",
    ") -> tfx.v1.dsl.Pipeline:\n",
    "\n",
    "\n",
    "    components = []\n",
    "\n",
    "  ##ExampleGen \n",
    "    output_config = tfx.proto.example_gen_pb2.Output(\n",
    "    split_config = tfx.proto.example_gen_pb2.SplitConfig(splits=[\n",
    "          tfx.proto.example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=4),\n",
    "          tfx.proto.example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "\n",
    "    ])\n",
    "    )\n",
    "\n",
    "    example_gen = tfx.components.CsvExampleGen(\n",
    "    input_base = data_path,\n",
    "    output_config = output_config\n",
    "    )\n",
    "\n",
    "  ##adding ExampleGen component \n",
    "    components.append(example_gen)\n",
    "\n",
    "  ## StatisticsGen\n",
    "    statistics_gen = tfx.components.StatisticsGen(\n",
    "        examples=example_gen.outputs['examples']\n",
    "    )\n",
    "\n",
    "    components.append(statistics_gen)\n",
    "\n",
    "  ##SchemaGen\n",
    "\n",
    "    if schema_path is None:\n",
    "        schema_gen = tfx.components.SchemaGen(statistics=statistics_gen.outputs['statistics'],\n",
    "                       infer_feature_shape=False)\n",
    "    \n",
    "    else:\n",
    "        schema_gen = tfx.components.SchemaGen(schema_file=schema_path)\n",
    "\n",
    "    components.append(schema_gen)\n",
    "\n",
    "\n",
    "    ##Example validator\n",
    "\n",
    "  \n",
    "    ##Transform\n",
    "    transform = tfx.components.Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        ##module_file='pipeline/preprocessing.py'\n",
    "        preprocessing_fn=preprocessing_fn\n",
    "    )\n",
    "\n",
    "    components.append(transform)\n",
    "    \n",
    "    if enable_tuning:\n",
    "        tuner = tfx.components.Tuner(\n",
    "            ##module_file=TRAINER_MODULE,\n",
    "            tuner_fn=tuner_fn,\n",
    "            examples=transform.outputs['transformed_examples'],\n",
    "            transform_graph=transform.outputs['transform_graph'],\n",
    "            train_args=train_args,\n",
    "            eval_args=eval_args\n",
    "        )\n",
    "        components.append(tuner)\n",
    "        \n",
    "    ##if tuner is not enabled = we already got  best parameters\n",
    "    ## import best parameters from the previous session\n",
    "    \n",
    "    ##TODO - figure out if there is a way to import the best parameters from the MD store\n",
    "    if not enable_tuning:\n",
    "        hparams_importer = tfx.v1.dsl.Importer(\n",
    "            source_uri = './tfx_pipeline_output/diamonds-pipeline/Tuner/best_hyperparameters/6',\n",
    "            artifact_type = tfx.v1.types.standard_artifacts.HyperParameters).with_id('import_hparams')\n",
    "        components.append(hparams_importer)\n",
    "\n",
    "    ##Trainer\n",
    "    trainer = tfx.components.Trainer(\n",
    "        ##module_file=TRAINER_MODULE,\n",
    "        run_fn=run_fn,\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        ##transformed_examples=transform.outputs['transformed_examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        hyperparameters=(tuner.outputs['best_hyperparameters'] if enable_tuning else hparams_importer.outputs['result']),  \n",
    "        ##train_args=trainer_pb2.TrainArgs(num_steps=3000),\n",
    "        ##eval_args=trainer_pb2.EvalArgs(num_steps=1000)\n",
    "        train_args=train_args,\n",
    "        eval_args=eval_args\n",
    "    )\n",
    "\n",
    "    components.append(trainer)\n",
    "\n",
    "    ##add more components as needed\n",
    "  \n",
    "    ##Evaluator\n",
    "    ## evaluates the new model\n",
    "    ##validates it against a base model if it is good enough to be pushed\n",
    " \n",
    "    metrics_specs= tfma.MetricsSpec(\n",
    "                metrics=[\n",
    "                    tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                    tfma.MetricConfig(class_name='MeanSquaredError',\n",
    "                                     threshold=tfma.MetricThreshold(\n",
    "                                         change_threshold=tfma.GenericChangeThreshold(\n",
    "                                         absolute={'value': -1e-10}, direction=tfma.MetricDirection.LOWER_IS_BETTER)))\n",
    "                ],\n",
    "    ##you can add threshold map for metrics used in the model training\n",
    "        thresholds = {\n",
    "            'root_mean_squared_error': tfma.MetricThreshold(\n",
    "                value_threshold=tfma.GenericValueThreshold(),\n",
    "                    ##you dont have to set an upper bound - will default to infinity\n",
    "                    ##upper_bound={'value': 2000}),\n",
    "                change_threshold=tfma.GenericChangeThreshold(\n",
    "                        direction=tfma.MetricDirection.LOWER_IS_BETTER,\n",
    "                        absolute={'value': 1e-10}\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[\n",
    "        # This assumes a serving model with signature 'serving_default'. If\n",
    "        # using estimator based EvalSavedModel, add signature_name: 'eval' and\n",
    "        # remove the label_key.\n",
    "            tfma.ModelSpec(\n",
    "                signature_name='serving_default',\n",
    "                label_key='price'\n",
    "            ##preprocessing_function_names=['transform_features'],\n",
    "                )\n",
    "        ],\n",
    "        metrics_specs=[metrics_specs],\n",
    "        slicing_specs=[\n",
    "            ##this is for the whole dataset, no slices\n",
    "            tfma.SlicingSpec(),\n",
    "            ##also specifying slices for some features \n",
    "            tfma.SlicingSpec(feature_keys=['cut']),\n",
    "            tfma.SlicingSpec(feature_keys=['color']),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ##resolve the previous blessed model\n",
    "    model_resolver=tfx.v1.dsl.Resolver(\n",
    "        strategy_class=tfx.v1.dsl.experimental.LatestBlessedModelStrategy,\n",
    "        model=tfx.v1.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "        model_blessing=tfx.v1.dsl.Channel(type=tfx.types.standard_artifacts.ModelBlessing)).with_id('latest_blessed_model_resolver')\n",
    "\n",
    "    components.append(model_resolver)\n",
    "\n",
    "    model_analyzer = tfx.components.Evaluator(\n",
    "        examples = example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        eval_config=eval_config\n",
    "    )\n",
    "    \n",
    "    components.append(model_analyzer)\n",
    "    \n",
    "    ##Pusher\n",
    "    pusher = tfx.components.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        model_blessing=model_analyzer.outputs['blessing'],\n",
    "        push_destination=tfx.v1.proto.PushDestination(\n",
    "            filesystem=tfx.v1.proto.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    components.append(pusher)\n",
    "\n",
    "    return tfx.v1.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      components=components,\n",
    "      metadata_connection_config=metadata_connection_config,\n",
    "      beam_pipeline_args=beam_pipeline_args,\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1647281498893,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "u196YqqVeSvw",
    "outputId": "5380f16f-b106-43f8-b2d0-55cbe4149228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting local_runner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile local_runner.py\n",
    "\n",
    "\"\"\"Define LocalDagRunner to run the pipeline locally.\"\"\"\n",
    "\n",
    "import os\n",
    "from absl import logging\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from pipeline import configs\n",
    "from pipeline import pipeline\n",
    "\n",
    "OUTPUT_DIR='.'\n",
    "\n",
    "PIPELINE_BUCKET = 'gs://diamonds_pipeline'\n",
    "\n",
    "PIPELINE_ROOT = os.path.join(OUTPUT_DIR, 'tfx_pipeline_output',\n",
    "                             configs.PIPELINE_NAME)\n",
    "METADATA_PATH = os.path.join(OUTPUT_DIR, 'tfx_metadata', configs.PIPELINE_NAME,\n",
    "                             'metadata.db')\n",
    "SERVING_MODEL_DIR = os.path.join(PIPELINE_BUCKET, 'serving_model')\n",
    "\n",
    "\n",
    "DATA_PATH = 'gs://diamonds_data'\n",
    "\n",
    "if not DATA_PATH:\n",
    "    DATA_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')\n",
    "\n",
    "def run():\n",
    "  \"\"\"Define a pipeline.\"\"\"\n",
    "\n",
    "  ##debug code\n",
    "  print(\"serving model dir: \", SERVING_MODEL_DIR)\n",
    "  print(\"metadata path: \", METADATA_PATH)\n",
    "\n",
    "  tfx.orchestration.LocalDagRunner().run(\n",
    "      pipeline.create_pipeline(\n",
    "          pipeline_name=configs.PIPELINE_NAME,\n",
    "          pipeline_root=PIPELINE_ROOT,\n",
    "          data_path=DATA_PATH,\n",
    "          # NOTE: Use `query` instead of `data_path` to use BigQueryExampleGen.\n",
    "          # query=configs.BIG_QUERY_QUERY,\n",
    "          # NOTE: Set the path of the customized schema if any.\n",
    "          # schema_path=generated_schema_path,\n",
    "          preprocessing_fn=configs.PREPROCESSING_FN,\n",
    "          run_fn=configs.RUN_FN,\n",
    "          tuner_fn=configs.TUNER_FN,\n",
    "          enable_tuning=False,\n",
    "          train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),\n",
    "          eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),\n",
    "          eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,\n",
    "          serving_model_dir=SERVING_MODEL_DIR,\n",
    "          # NOTE: Provide GCP configs to use BigQuery with Beam DirectRunner.\n",
    "          # beam_pipeline_args=configs.\n",
    "          # BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS,\n",
    "          metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(METADATA_PATH))\n",
    "      )\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  logging.set_verbosity(logging.INFO)\n",
    "  run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10660,
     "status": "ok",
     "timestamp": 1647282323648,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "fBzmf-5Rf1fF",
    "outputId": "036bfe49-9176-4f7e-adac-e2c021b36cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Compiling pipeline\n",
      "Detected Local.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "serving model dir:  gs://diamonds_pipeline/serving_model\n",
      "metadata path:  ./tfx_metadata/diamonds-pipeline/metadata.db\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Pipeline compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --pipeline_path local_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10461,
     "status": "ok",
     "timestamp": 1647279880249,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "FLkufPVJxAeT",
    "outputId": "d881fcbd-7801-4e8b-f74c-860e1e438cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating pipeline\n",
      "Detected Local.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "serving model dir:  gs://diamonds_pipeline/serving_model/1\n",
      "metadata path:  ./tfx_metadata/diamonds-pipeline/metadata.db\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Pipeline \"diamonds-pipeline\" already exists.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline create --pipeline_path local_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10567,
     "status": "ok",
     "timestamp": 1647282340457,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "lyP9YBtiiuMO",
    "outputId": "48683be6-136c-49bd-a01b-b86e1d85d1a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Updating pipeline\n",
      "Detected Local.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "serving model dir:  gs://diamonds_pipeline/serving_model\n",
      "metadata path:  ./tfx_metadata/diamonds-pipeline/metadata.db\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Pipeline \"diamonds-pipeline\" updated successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update --pipeline_path local_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 349777,
     "status": "ok",
     "timestamp": 1647282698510,
     "user": {
      "displayName": "Milan Jendrisek",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09200476014820705300"
     },
     "user_tz": 360
    },
    "id": "SyoGRB4xxX9y",
    "outputId": "53298ae5-9364-459d-cfce-aa6d431dfc23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating a run for pipeline: diamonds-pipeline\n",
      "Detected Local.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "serving model dir:  gs://diamonds_pipeline/serving_model\n",
      "metadata path:  ./tfx_metadata/diamonds-pipeline/metadata.db\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Evaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Pusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"SchemaGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.schema_gen.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Trainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Transform\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.transform.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"./tfx_metadata/diamonds-pipeline/metadata.db\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"./tfx_metadata/diamonds-pipeline/metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"gs://diamonds_data\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 1\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/CsvExampleGen/examples/1\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2772123,xor_checksum:1646244763,sum_checksum:1646244763\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_file_format': 5, 'input_base': 'gs://diamonds_data', 'output_data_format': 6, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 4,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:2772123,xor_checksum:1646244763,sum_checksum:1646244763'}, execution_output_uri='./tfx_pipeline_output/diamonds-pipeline/CsvExampleGen/.system/executor_execution/1/executor_output.pb', stateful_working_dir='./tfx_pipeline_output/diamonds-pipeline/CsvExampleGen/.system/stateful_working_dir/2022-03-30T22:49:00.477453', tmp_dir='./tfx_pipeline_output/diamonds-pipeline/CsvExampleGen/.system/executor_execution/1/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"gs://diamonds_data\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"diamonds-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T22:49:00.477453')\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Processing input csv data gs://diamonds_data/* to TFExample.\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 1 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/CsvExampleGen/examples/1\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2772123,xor_checksum:1646244763,sum_checksum:1646244763\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 1\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n",
      "INFO:absl:Component import_hparams is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "  }\n",
      "  id: \"import_hparams\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.import_hparams\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"result\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"artifact_uri\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"./tfx_pipeline_output/diamonds-pipeline/Tuner/best_hyperparameters/6\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"reimport\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Running as an importer node.\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Processing source uri: ./tfx_pipeline_output/diamonds-pipeline/Tuner/best_hyperparameters/6, properties: {}, custom_properties: {}\n",
      "INFO:absl:Component import_hparams is finished.\n",
      "INFO:absl:Component latest_blessed_model_resolver is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"latest_blessed_model_resolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.latest_blessed_model_resolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"model\"\n",
      "      input_keys: \"model_blessing\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Running as an resolver node.\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:Artifact type Model is not found in MLMD.\n",
      "WARNING:absl:Artifact type ModelBlessing is not found in MLMD.\n",
      "INFO:absl:Component latest_blessed_model_resolver is finished.\n",
      "INFO:absl:Component StatisticsGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"SchemaGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 4\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/CsvExampleGen/examples/1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2772123,xor_checksum:1646244763,sum_checksum:1646244763\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680555890\n",
      "last_update_time_since_epoch: 1648680555890\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/StatisticsGen/statistics/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='./tfx_pipeline_output/diamonds-pipeline/StatisticsGen/.system/executor_execution/4/executor_output.pb', stateful_working_dir='./tfx_pipeline_output/diamonds-pipeline/StatisticsGen/.system/stateful_working_dir/2022-03-30T22:49:00.477453', tmp_dir='./tfx_pipeline_output/diamonds-pipeline/StatisticsGen/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"SchemaGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"diamonds-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T22:49:00.477453')\n",
      "INFO:absl:Generating statistics for split train.\n",
      "INFO:absl:Statistics for split train written to ./tfx_pipeline_output/diamonds-pipeline/StatisticsGen/statistics/4/Split-train.\n",
      "INFO:absl:Generating statistics for split eval.\n",
      "INFO:absl:Statistics for split eval written to ./tfx_pipeline_output/diamonds-pipeline/StatisticsGen/statistics/4/Split-eval.\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 4 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/StatisticsGen/statistics/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}) for execution 4\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component StatisticsGen is finished.\n",
      "INFO:absl:Component SchemaGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.schema_gen.component.SchemaGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"SchemaGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.SchemaGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"infer_feature_shape\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 5\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={'statistics': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/StatisticsGen/statistics/4\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680561374\n",
      "last_update_time_since_epoch: 1648680561374\n",
      ", artifact_type: id: 20\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'schema': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/SchemaGen/schema/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:SchemaGen:schema:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Schema\"\n",
      ")]}), exec_properties={'infer_feature_shape': 0, 'exclude_splits': '[]'}, execution_output_uri='./tfx_pipeline_output/diamonds-pipeline/SchemaGen/.system/executor_execution/5/executor_output.pb', stateful_working_dir='./tfx_pipeline_output/diamonds-pipeline/SchemaGen/.system/stateful_working_dir/2022-03-30T22:49:00.477453', tmp_dir='./tfx_pipeline_output/diamonds-pipeline/SchemaGen/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.schema_gen.component.SchemaGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"SchemaGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.SchemaGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"infer_feature_shape\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"diamonds-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T22:49:00.477453')\n",
      "INFO:absl:Processing schema from statistics for split train.\n",
      "INFO:absl:Processing schema from statistics for split eval.\n",
      "INFO:absl:Schema written to ./tfx_pipeline_output/diamonds-pipeline/SchemaGen/schema/5/schema.pbtxt.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 5 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'schema': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/SchemaGen/schema/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:SchemaGen:schema:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Schema\"\n",
      ")]}) for execution 5\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component SchemaGen is finished.\n",
      "INFO:absl:Component Transform is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"Transform\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.Transform\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"preprocessing_fn\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"model.preprocessing.preprocessing_fn\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 6\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=6, input_dict={'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/CsvExampleGen/examples/1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2772123,xor_checksum:1646244763,sum_checksum:1646244763\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680555890\n",
      "last_update_time_since_epoch: 1648680555890\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'schema': [Artifact(artifact: id: 4\n",
      "type_id: 22\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/SchemaGen/schema/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:SchemaGen:schema:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680561439\n",
      "last_update_time_since_epoch: 1648680561439\n",
      ", artifact_type: id: 22\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pre_transform_stats': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/pre_transform_stats/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:pre_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_stats': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/post_transform_stats/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:post_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_anomalies': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/post_transform_anomalies/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:post_transform_anomalies:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'pre_transform_schema': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/pre_transform_schema/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:pre_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'transformed_examples': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/transformed_examples/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:transformed_examples:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'transform_graph': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/transform_graph/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:transform_graph:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"TransformGraph\"\n",
      ")], 'post_transform_schema': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/post_transform_schema/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:post_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'updated_analyzer_cache': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/updated_analyzer_cache/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:updated_analyzer_cache:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"TransformCache\"\n",
      ")]}), exec_properties={'custom_config': 'null', 'preprocessing_fn': 'model.preprocessing.preprocessing_fn', 'disable_statistics': 0, 'force_tf_compat_v1': 0}, execution_output_uri='./tfx_pipeline_output/diamonds-pipeline/Transform/.system/executor_execution/6/executor_output.pb', stateful_working_dir='./tfx_pipeline_output/diamonds-pipeline/Transform/.system/stateful_working_dir/2022-03-30T22:49:00.477453', tmp_dir='./tfx_pipeline_output/diamonds-pipeline/Transform/.system/executor_execution/6/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"Transform\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.Transform\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"preprocessing_fn\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"model.preprocessing.preprocessing_fn\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"diamonds-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T22:49:00.477453')\n",
      "INFO:absl:Analyze the 'train' split and transform all splits when splits_config is not set.\n",
      "INFO:absl:udf_utils.get_fn {'module_file': None, 'module_path': None, 'preprocessing_fn': 'model.preprocessing.preprocessing_fn'} 'preprocessing_fn'\n",
      "INFO:absl:Feature clarity has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature color has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature cut has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature carat has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature depth has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature price has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature table has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature x has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature y has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature z has no shape. Setting to VarLenSparseTensor.\n",
      "2022-03-30 22:49:21.690085: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-03-30 22:49:21.690158: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-30 22:49:21.690190: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-2-7-20220224-172650): /proc/driver/nvidia/version does not exist\n",
      "2022-03-30 22:49:21.690455: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:325: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:325: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "INFO:absl:Feature clarity has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature color has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature cut has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature carat has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature depth has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature price has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature table has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature x has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature y has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature z has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature clarity has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature color has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature cut has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature carat has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature depth has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature price has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature table has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature x has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature y has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature z has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature clarity has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature color has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature cut has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature carat has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature depth has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature price has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature table has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature x has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature y has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature z has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature clarity has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature color has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature cut has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature carat has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature depth has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature price has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature table has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature x has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature y has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature z has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature clarity has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature color has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature cut has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature carat has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature depth has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature price has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature table has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature x has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature y has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature z has no shape. Setting to VarLenSparseTensor.\n",
      "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_2/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_2/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "INFO:absl:Feature clarity has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature color has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature cut has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature carat has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature depth has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature price has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature table has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature x has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature y has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature z has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature clarity has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature color has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature cut has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature carat has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature depth has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature price has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature table has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature x has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature y has no shape. Setting to VarLenSparseTensor.\n",
      "INFO:absl:Feature z has no shape. Setting to VarLenSparseTensor.\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "2022-03-30 22:49:32.604824: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 6 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pre_transform_stats': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/pre_transform_stats/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:pre_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_stats': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/post_transform_stats/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:post_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_anomalies': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/post_transform_anomalies/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:post_transform_anomalies:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'pre_transform_schema': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/pre_transform_schema/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:pre_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'transformed_examples': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/transformed_examples/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:transformed_examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'transform_graph': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/transform_graph/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:transform_graph:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"TransformGraph\"\n",
      ")], 'post_transform_schema': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/post_transform_schema/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:post_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'updated_analyzer_cache': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/updated_analyzer_cache/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:updated_analyzer_cache:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"TransformCache\"\n",
      ")]}) for execution 6\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Transform is finished.\n",
      "INFO:absl:Component Trainer is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transformed_examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"import_hparams\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.import_hparams\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"HyperParameters\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"TransformGraph\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transform_graph\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 1000\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"run_fn\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"model.model.run_fn\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 5000\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "upstream_nodes: \"Transform\"\n",
      "upstream_nodes: \"import_hparams\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 7\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=7, input_dict={'hyperparameters': [Artifact(artifact: id: 2\n",
      "type_id: 17\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/Tuner/best_hyperparameters/6\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680555928\n",
      "last_update_time_since_epoch: 1648680555928\n",
      ", artifact_type: id: 17\n",
      "name: \"HyperParameters\"\n",
      ")], 'examples': [Artifact(artifact: id: 9\n",
      "type_id: 15\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/transformed_examples/6\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:transformed_examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680591713\n",
      "last_update_time_since_epoch: 1648680591713\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'schema': [Artifact(artifact: id: 4\n",
      "type_id: 22\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/SchemaGen/schema/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:SchemaGen:schema:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680561439\n",
      "last_update_time_since_epoch: 1648680561439\n",
      ", artifact_type: id: 22\n",
      "name: \"Schema\"\n",
      ")], 'transform_graph': [Artifact(artifact: id: 10\n",
      "type_id: 25\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/Transform/transform_graph/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Transform:transform_graph:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680591713\n",
      "last_update_time_since_epoch: 1648680591713\n",
      ", artifact_type: id: 25\n",
      "name: \"TransformGraph\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Trainer/model_run/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Trainer:model_run:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")], 'model': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Trainer/model/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'train_args': '{\\n  \"num_steps\": 5000\\n}', 'eval_args': '{\\n  \"num_steps\": 1000\\n}', 'custom_config': 'null', 'run_fn': 'model.model.run_fn'}, execution_output_uri='./tfx_pipeline_output/diamonds-pipeline/Trainer/.system/executor_execution/7/executor_output.pb', stateful_working_dir='./tfx_pipeline_output/diamonds-pipeline/Trainer/.system/stateful_working_dir/2022-03-30T22:49:00.477453', tmp_dir='./tfx_pipeline_output/diamonds-pipeline/Trainer/.system/executor_execution/7/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transformed_examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"import_hparams\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.import_hparams\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"HyperParameters\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"TransformGraph\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transform_graph\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 1000\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"run_fn\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"model.model.run_fn\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 5000\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "upstream_nodes: \"Transform\"\n",
      "upstream_nodes: \"import_hparams\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"diamonds-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T22:49:00.477453')\n",
      "INFO:absl:Train on the 'train' split when train_args.splits is not set.\n",
      "INFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "INFO:absl:udf_utils.get_fn {'train_args': '{\\n  \"num_steps\": 5000\\n}', 'eval_args': '{\\n  \"num_steps\": 1000\\n}', 'custom_config': 'null', 'run_fn': 'model.model.run_fn'} 'run_fn'\n",
      "INFO:absl:Training model.\n",
      "INFO:absl:Feature carat_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature clarity_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature color_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature cut_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature depth_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature price_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature table_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature x_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature y_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature z_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature carat_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature clarity_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature color_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature cut_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature depth_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature price_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature table_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature x_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature y_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature z_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature carat_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature clarity_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature color_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature cut_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature depth_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature price_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature table_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature x_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature y_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature z_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature carat_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature clarity_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature color_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature cut_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature depth_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature price_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature table_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature x_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature y_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature z_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:HyperParameters for training: {'space': [{'class_name': 'Choice', 'config': {'name': 'learning_rate', 'default': 0.001, 'conditions': [], 'values': [0.1, 0.01, 0.001], 'ordered': True}}, {'class_name': 'Int', 'config': {'name': 'n_layers', 'default': 1, 'conditions': [], 'min_value': 1, 'max_value': 2, 'step': 1, 'sampling': None}}, {'class_name': 'Int', 'config': {'name': '1_layer_n_units_1', 'default': 8, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'n_layers', 'values': [1]}}], 'min_value': 8, 'max_value': 128, 'step': 8, 'sampling': None}}, {'class_name': 'Int', 'config': {'name': '2_layer_n_units_1', 'default': 8, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'n_layers', 'values': [2]}}], 'min_value': 8, 'max_value': 128, 'step': 8, 'sampling': None}}, {'class_name': 'Int', 'config': {'name': '2_layer_n_units_2', 'default': 8, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'n_layers', 'values': [2]}}], 'min_value': 8, 'max_value': 128, 'step': 8, 'sampling': None}}], 'values': {'learning_rate': 0.1, 'n_layers': 2, '2_layer_n_units_1': 128, '2_layer_n_units_2': 128}}\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "INFO:absl:Model: \"model\"\n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl: Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "INFO:absl:==================================================================================================\n",
      "INFO:absl: carat_xf (InputLayer)          [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: depth_xf (InputLayer)          [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: table_xf (InputLayer)          [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: x_xf (InputLayer)              [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: y_xf (InputLayer)              [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: z_xf (InputLayer)              [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: clarity_xf (InputLayer)        [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: color_xf (InputLayer)          [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: cut_xf (InputLayer)            [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: concatenate (Concatenate)      (None, 9)            0           ['carat_xf[0][0]',               \n",
      "INFO:absl:                                                                  'depth_xf[0][0]',               \n",
      "INFO:absl:                                                                  'table_xf[0][0]',               \n",
      "INFO:absl:                                                                  'x_xf[0][0]',                   \n",
      "INFO:absl:                                                                  'y_xf[0][0]',                   \n",
      "INFO:absl:                                                                  'z_xf[0][0]',                   \n",
      "INFO:absl:                                                                  'clarity_xf[0][0]',             \n",
      "INFO:absl:                                                                  'color_xf[0][0]',               \n",
      "INFO:absl:                                                                  'cut_xf[0][0]']                 \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: dense_1 (Dense)                (None, 128)          1280        ['concatenate[0][0]']            \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: dense_2 (Dense)                (None, 1)            129         ['dense_1[0][0]']                \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl:==================================================================================================\n",
      "INFO:absl:Total params: 1,409\n",
      "INFO:absl:Trainable params: 1,409\n",
      "INFO:absl:Non-trainable params: 0\n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "2022-03-30 22:49:52.280019: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 20s 3ms/step - loss: 1673110.7500 - root_mean_squared_error: 1293.4878 - val_loss: 1771620.1250 - val_root_mean_squared_error: 1331.0222\n",
      "Epoch 2/10\n",
      "4996/5000 [============================>.] - ETA: 0s - loss: 1195593.7500 - root_mean_squared_error: 1093.43212022-03-30 22:50:25.804164: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1195782.5000 - root_mean_squared_error: 1093.5184 - val_loss: 1352370.8750 - val_root_mean_squared_error: 1162.9148\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 844736.6250 - root_mean_squared_error: 919.0955 - val_loss: 1743234.0000 - val_root_mean_squared_error: 1320.3159\n",
      "Epoch 4/10\n",
      "4984/5000 [============================>.] - ETA: 0s - loss: 792700.5000 - root_mean_squared_error: 890.33732022-03-30 22:50:57.780625: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 791154.1250 - root_mean_squared_error: 889.4684 - val_loss: 1264429.5000 - val_root_mean_squared_error: 1124.4685\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 729905.4375 - root_mean_squared_error: 854.3450 - val_loss: 883663.0625 - val_root_mean_squared_error: 940.0336\n",
      "Epoch 6/10\n",
      "4999/5000 [============================>.] - ETA: 0s - loss: 638177.2500 - root_mean_squared_error: 798.86002022-03-30 22:51:30.480449: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 638373.8750 - root_mean_squared_error: 798.9830 - val_loss: 919346.8750 - val_root_mean_squared_error: 958.8258\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 605571.3750 - root_mean_squared_error: 778.1847 - val_loss: 847140.6250 - val_root_mean_squared_error: 920.4024\n",
      "Epoch 8/10\n",
      "4997/5000 [============================>.] - ETA: 0s - loss: 600641.8750 - root_mean_squared_error: 775.01092022-03-30 22:52:03.807350: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 600621.4375 - root_mean_squared_error: 774.9977 - val_loss: 885840.2500 - val_root_mean_squared_error: 941.1909\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 556191.1250 - root_mean_squared_error: 745.7822 - val_loss: 911086.8125 - val_root_mean_squared_error: 954.5087\n",
      "Epoch 10/10\n",
      "4983/5000 [============================>.] - ETA: 0s - loss: 505880.5938 - root_mean_squared_error: 711.25282022-03-30 22:52:37.235660: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "5000/5000 [==============================] - 17s 3ms/step - loss: 505726.8125 - root_mean_squared_error: 711.1447 - val_loss: 1223853.5000 - val_root_mean_squared_error: 1106.2792\n",
      "INFO:absl:Training complete. Model written to ./tfx_pipeline_output/diamonds-pipeline/Trainer/model/7/Format-Serving. ModelRun written to ./tfx_pipeline_output/diamonds-pipeline/Trainer/model_run/7\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 7 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Trainer/model_run/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Trainer:model_run:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")], 'model': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Trainer/model/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 7\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Trainer is finished.\n",
      "INFO:absl:Component Evaluator is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "    base_type: EVALUATE\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"latest_blessed_model_resolver\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.latest_blessed_model_resolver\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"MeanSquaredError\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": -1e-10,\\n              \\\"direction\\\": \\\"LOWER_IS_BETTER\\\"\\n            }\\n          }\\n        }\\n      ],\\n      \\\"thresholds\\\": {\\n        \\\"root_mean_squared_error\\\": {\\n          \\\"change_threshold\\\": {\\n            \\\"absolute\\\": 1e-10,\\n            \\\"direction\\\": \\\"LOWER_IS_BETTER\\\"\\n          },\\n          \\\"value_threshold\\\": {}\\n        }\\n      }\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"price\\\",\\n      \\\"signature_name\\\": \\\"serving_default\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"cut\\\"\\n      ]\\n    },\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"color\\\"\\n      ]\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"fairness_indicator_thresholds\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "upstream_nodes: \"latest_blessed_model_resolver\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 8\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/CsvExampleGen/examples/1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2772123,xor_checksum:1646244763,sum_checksum:1646244763\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680555890\n",
      "last_update_time_since_epoch: 1648680555890\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'model': [Artifact(artifact: id: 14\n",
      "type_id: 29\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/Trainer/model/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680800930\n",
      "last_update_time_since_epoch: 1648680800930\n",
      ", artifact_type: id: 29\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'baseline_model': []}, output_dict=defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Evaluator/evaluation/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Evaluator:evaluation:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")], 'blessing': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Evaluator/blessing/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Evaluator:blessing:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")]}), exec_properties={'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"MeanSquaredError\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": -1e-10,\\n              \"direction\": \"LOWER_IS_BETTER\"\\n            }\\n          }\\n        }\\n      ],\\n      \"thresholds\": {\\n        \"root_mean_squared_error\": {\\n          \"change_threshold\": {\\n            \"absolute\": 1e-10,\\n            \"direction\": \"LOWER_IS_BETTER\"\\n          },\\n          \"value_threshold\": {}\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"price\",\\n      \"signature_name\": \"serving_default\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"cut\"\\n      ]\\n    },\\n    {\\n      \"feature_keys\": [\\n        \"color\"\\n      ]\\n    }\\n  ]\\n}'}, execution_output_uri='./tfx_pipeline_output/diamonds-pipeline/Evaluator/.system/executor_execution/8/executor_output.pb', stateful_working_dir='./tfx_pipeline_output/diamonds-pipeline/Evaluator/.system/stateful_working_dir/2022-03-30T22:49:00.477453', tmp_dir='./tfx_pipeline_output/diamonds-pipeline/Evaluator/.system/executor_execution/8/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "    base_type: EVALUATE\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"latest_blessed_model_resolver\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.latest_blessed_model_resolver\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"MeanSquaredError\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": -1e-10,\\n              \\\"direction\\\": \\\"LOWER_IS_BETTER\\\"\\n            }\\n          }\\n        }\\n      ],\\n      \\\"thresholds\\\": {\\n        \\\"root_mean_squared_error\\\": {\\n          \\\"change_threshold\\\": {\\n            \\\"absolute\\\": 1e-10,\\n            \\\"direction\\\": \\\"LOWER_IS_BETTER\\\"\\n          },\\n          \\\"value_threshold\\\": {}\\n        }\\n      }\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"price\\\",\\n      \\\"signature_name\\\": \\\"serving_default\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"cut\\\"\\n      ]\\n    },\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"color\\\"\\n      ]\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"fairness_indicator_thresholds\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "upstream_nodes: \"latest_blessed_model_resolver\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"diamonds-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T22:49:00.477453')\n",
      "INFO:absl:udf_utils.get_fn {'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"MeanSquaredError\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": -1e-10,\\n              \"direction\": \"LOWER_IS_BETTER\"\\n            }\\n          }\\n        }\\n      ],\\n      \"thresholds\": {\\n        \"root_mean_squared_error\": {\\n          \"change_threshold\": {\\n            \"absolute\": 1e-10,\\n            \"direction\": \"LOWER_IS_BETTER\"\\n          },\\n          \"value_threshold\": {}\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"price\",\\n      \"signature_name\": \"serving_default\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"cut\"\\n      ]\\n    },\\n    {\\n      \"feature_keys\": [\\n        \"color\"\\n      ]\\n    }\\n  ]\\n}'} 'custom_eval_shared_model'\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  signature_name: \"serving_default\"\n",
      "  label_key: \"price\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"cut\"\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"color\"\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"MeanSquaredError\"\n",
      "    threshold {\n",
      "    }\n",
      "  }\n",
      "  thresholds {\n",
      "    key: \"root_mean_squared_error\"\n",
      "    value {\n",
      "      value_threshold {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using ./tfx_pipeline_output/diamonds-pipeline/Trainer/model/7/Format-Serving as  model.\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f7448239850> and <keras.engine.input_layer.InputLayer object at 0x7f7486095110>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f7448239850> and <keras.engine.input_layer.InputLayer object at 0x7f7486095110>).\n",
      "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
      "INFO:absl:Evaluating model.\n",
      "INFO:absl:udf_utils.get_fn {'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"MeanSquaredError\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": -1e-10,\\n              \"direction\": \"LOWER_IS_BETTER\"\\n            }\\n          }\\n        }\\n      ],\\n      \"thresholds\": {\\n        \"root_mean_squared_error\": {\\n          \"change_threshold\": {\\n            \"absolute\": 1e-10,\\n            \"direction\": \"LOWER_IS_BETTER\"\\n          },\\n          \"value_threshold\": {}\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"price\",\\n      \"signature_name\": \"serving_default\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"cut\"\\n      ]\\n    },\\n    {\\n      \"feature_keys\": [\\n        \"color\"\\n      ]\\n    }\\n  ]\\n}'} 'custom_extractors'\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  signature_name: \"serving_default\"\n",
      "  label_key: \"price\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"cut\"\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"color\"\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"MeanSquaredError\"\n",
      "    threshold {\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "  thresholds {\n",
      "    key: \"root_mean_squared_error\"\n",
      "    value {\n",
      "      value_threshold {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  signature_name: \"serving_default\"\n",
      "  label_key: \"price\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"cut\"\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"color\"\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"MeanSquaredError\"\n",
      "    threshold {\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "  thresholds {\n",
      "    key: \"root_mean_squared_error\"\n",
      "    value {\n",
      "      value_threshold {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  signature_name: \"serving_default\"\n",
      "  label_key: \"price\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"cut\"\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"color\"\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"MeanSquaredError\"\n",
      "    threshold {\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "  thresholds {\n",
      "    key: \"root_mean_squared_error\"\n",
      "    value {\n",
      "      value_threshold {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f7449bb3bd0> and <keras.engine.input_layer.InputLayer object at 0x7f748030d510>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f7449bb3bd0> and <keras.engine.input_layer.InputLayer object at 0x7f748030d510>).\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f744ad04cd0> and <keras.engine.input_layer.InputLayer object at 0x7f7460165190>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f744ad04cd0> and <keras.engine.input_layer.InputLayer object at 0x7f7460165190>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f744b589b90> and <keras.engine.input_layer.InputLayer object at 0x7f744b2d7750>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f744b589b90> and <keras.engine.input_layer.InputLayer object at 0x7f744b2d7750>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f744b3ab450> and <keras.engine.input_layer.InputLayer object at 0x7f744b3a7750>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f744b3ab450> and <keras.engine.input_layer.InputLayer object at 0x7f744b3a7750>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f743562eed0> and <keras.engine.input_layer.InputLayer object at 0x7f743560dad0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f743562eed0> and <keras.engine.input_layer.InputLayer object at 0x7f743560dad0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f743513ef10> and <keras.engine.input_layer.InputLayer object at 0x7f7435164510>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f743513ef10> and <keras.engine.input_layer.InputLayer object at 0x7f7435164510>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f7435183f10> and <keras.engine.input_layer.InputLayer object at 0x7f748034c410>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f7435183f10> and <keras.engine.input_layer.InputLayer object at 0x7f748034c410>).\n",
      "INFO:absl:Evaluation complete. Results written to ./tfx_pipeline_output/diamonds-pipeline/Evaluator/evaluation/8.\n",
      "INFO:absl:Checking validation results.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:109: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:109: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:absl:Blessing result True written to ./tfx_pipeline_output/diamonds-pipeline/Evaluator/blessing/8.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 8 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Evaluator/evaluation/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Evaluator:evaluation:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")], 'blessing': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Evaluator/blessing/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Evaluator:blessing:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")]}) for execution 8\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Evaluator is finished.\n",
      "INFO:absl:Component Pusher is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"gs://diamonds_pipeline/serving_model\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 9\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'model_blessing': [Artifact(artifact: id: 16\n",
      "type_id: 32\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/Evaluator/blessing/8\"\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"./tfx_pipeline_output/diamonds-pipeline/Trainer/model/7\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 14\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Evaluator:blessing:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680822112\n",
      "last_update_time_since_epoch: 1648680822112\n",
      ", artifact_type: id: 32\n",
      "name: \"ModelBlessing\"\n",
      ")], 'model': [Artifact(artifact: id: 14\n",
      "type_id: 29\n",
      "uri: \"./tfx_pipeline_output/diamonds-pipeline/Trainer/model/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648680800930\n",
      "last_update_time_since_epoch: 1648680800930\n",
      ", artifact_type: id: 29\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Pusher/pushed_model/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Pusher:pushed_model:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'custom_config': 'null', 'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"gs://diamonds_pipeline/serving_model\"\\n  }\\n}'}, execution_output_uri='./tfx_pipeline_output/diamonds-pipeline/Pusher/.system/executor_execution/9/executor_output.pb', stateful_working_dir='./tfx_pipeline_output/diamonds-pipeline/Pusher/.system/stateful_working_dir/2022-03-30T22:49:00.477453', tmp_dir='./tfx_pipeline_output/diamonds-pipeline/Pusher/.system/executor_execution/9/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T22:49:00.477453\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"diamonds-pipeline.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T22:49:00.477453\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"diamonds-pipeline.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"gs://diamonds_pipeline/serving_model\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"diamonds-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T22:49:00.477453')\n",
      "INFO:absl:Model version: 1648680822\n",
      "INFO:absl:Model written to serving path gs://diamonds_pipeline/serving_model/1648680822.\n",
      "INFO:absl:Model pushed to ./tfx_pipeline_output/diamonds-pipeline/Pusher/pushed_model/9.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 9 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"./tfx_pipeline_output/diamonds-pipeline/Pusher/pushed_model/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"diamonds-pipeline:2022-03-30T22:49:00.477453:Pusher:pushed_model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 9\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Pusher is finished.\n"
     ]
    }
   ],
   "source": [
    "!tfx run create --pipeline_name diamonds-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6/0b/bnU759hXE+42I3vn",
   "name": "diamonds-tfx-pipeline-complete.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
